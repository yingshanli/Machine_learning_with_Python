{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Stochastic Gradient Descent\n",
    "\n",
    "The goal of this notebook is to study the following two aspects of the Gradient Descent algorithm for Multi-class Logistic Regression.\n",
    "\n",
    "- Stochastic Gradient Descent \n",
    "- Regularization via Early Stopping\n",
    "\n",
    "\n",
    "### <font color=red> Early Stopping</font>\n",
    "\n",
    "Early stopping is a regularization technique for iterative optimization algorithms such as Graient Descent that stops training as soon as the validation error reaches a minimum. \n",
    "\n",
    "Because we start with weights almost zero and they move away as training continues, stopping early corresponds to a model with more weights close to zero and effectively fewer parameters.\n",
    "\n",
    "### Early Stopping Curve:\n",
    "\n",
    "With Stochastic (and Mini-batch) Gradient Descent, the error vs iterations (epochs) curves are not so smooth.\n",
    "\n",
    "It may be hard to know whether we have reached the minimum or not. \n",
    "\n",
    "One solution is to stop only after the validation error has been above the minimum for some time (when we are confident that the model will not do any better), then roll back the model parameters to the point where the validation error was at a minimum.\n",
    "\n",
    "More on Stochastic Gradient Descent:\n",
    "https://scikit-learn.org/stable/modules/sgd.html#sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "\n",
    "We will use the iris dataset, which is a multivariate data set. \n",
    "\n",
    "This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica\n",
    "\n",
    "There are 4 features: \n",
    "- sepal length (cm)\n",
    "- sepal width (cm)\n",
    "- petal length (cm)\n",
    "- petal width (cm)\n",
    "\n",
    "Total number of samples: 150\n",
    "\n",
    "The dataset is also known as Fisher's Iris data set as it was introduced by the British statistician and biologist Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\".\n",
    "\n",
    "\n",
    "<img src=\"https://cse.unl.edu/~hasan/IrisFlowers.png\",width=800,height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'filename', 'DESCR', 'feature_names'])\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "print(iris.keys())\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "print(iris.data.shape)\n",
    "\n",
    "#print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Matrix (X) and the Label Vector (y)\n",
    "\n",
    "We can use all features or a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "\n",
    "The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. \n",
    "\n",
    "At the opposite extreme, Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients based only on that single instance. \n",
    "\n",
    "Obviously this makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration.\n",
    "\n",
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. \n",
    "\n",
    "Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down. So once the algorithm stops, the final parameter values are good, but not optimal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn SGDClassifier\n",
    "\n",
    "\n",
    "The SGDClassifier implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.\n",
    "\n",
    "The concrete loss function can be set via the loss parameter. SGDClassifier supports the following loss functions:\n",
    "\n",
    "- loss=\"hinge\": (soft-margin) linear Support Vector Machine\n",
    "- loss=\"modified_huber\": smoothed hinge loss\n",
    "- loss=\"log\": logistic regression\n",
    "\n",
    "For implementing SGD for Logistic Regression, we will use the \"log\" loss. The 'log' loss gives logistic regression, a probabilistic classifier.\n",
    "\n",
    "Using loss=\"log\" enables the predict_proba method, which gives a vector of probability estimates per sample.\n",
    "\n",
    "\n",
    "\n",
    "We need to set the following attributes to train a SGDClassifier.\n",
    "\n",
    "\n",
    "- penalty : ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’\n",
    "    -- The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.\n",
    "    \n",
    "\n",
    "- alpha : Constant that multiplies the regularization term. Defaults to 0.0001 \n",
    "\n",
    "\n",
    "- l1_ratio : The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.\n",
    "\n",
    "\n",
    "- max_iter : The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit. Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.\n",
    "\n",
    "\n",
    "- tol : The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol). Defaults to 1e-3 from 0.21.\n",
    "\n",
    "\n",
    "- random_state : The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "\n",
    "\n",
    "- learning_rate : The learning rate schedule:\n",
    "\n",
    "    -- ‘constant’: eta = eta0\n",
    "\n",
    "    --‘optimal’: [default] eta = 1.0 / (alpha * (t + t0)) where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
    "\n",
    "    --‘invscaling’: eta = eta0 / pow(t, power_t)\n",
    "\n",
    "    --‘adaptive’: eta = eta0, as long as the training keeps decreasing. Each time n_iter_no_change consecutive epochs fail to decrease the training loss by tol or fail to increase validation score by tol if early_stopping is True, the current learning rate is divided by 5.\n",
    "\n",
    "\n",
    "- eta0 : The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by the default schedule ‘optimal’.\n",
    "\n",
    "\n",
    "\n",
    "- early_stopping : Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.\n",
    "\n",
    "\n",
    "- n_iter_no_change : Number of iterations with no improvement to wait before early stopping.\n",
    "\n",
    "More detail: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier for Multi-Class Classification\n",
    "\n",
    "SGDClassifier supports multi-class classification by combining multiple binary classifiers in a “one versus all” (OvA) scheme. \n",
    "\n",
    "For each of the  classes, a binary classifier is learned that discriminates between that and all other classes. \n",
    "\n",
    "At testing time, we compute the confidence score (i.e. the signed distances to the hyperplane) for each classifier and choose the class with the highest confidence. \n",
    "\n",
    "Note that the Logistic Regression SGDClassifier does not use the Softmax regression technique for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation of the Stochastic Gradient Descent for Logistic Regression\n",
    "\n",
    "We will investigate the SGDClassifier:\n",
    "- Without Early Stopping\n",
    "- With Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Without Early Stopping\n",
    "\n",
    "\n",
    "We will implement the SGDClassifier without early stopping.\n",
    "\n",
    "For larger dataset, even with l2 regularization in effect, the weights will be larger.\n",
    "\n",
    "\n",
    "First, we need to find the optimal hyperparameters via Gridsearch.\n",
    "\n",
    "## Model Selection: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.950000\n",
      "Optimal Hyperparameter Values:  {'max_iter': 3000, 'alpha': 0.001, 'tol': 1e-08}\n",
      "CPU times: user 847 ms, sys: 8.52 ms, total: 855 ms\n",
      "Wall time: 858 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'alpha': [0.01, 0.001, 0.0007, 0.0005, 0.0001],\n",
    "              'tol': [1e-3, 1e-5, 1e-8], 'max_iter':[100, 500, 1000, 3000, 7000]}\n",
    "              \n",
    "\n",
    "sgd_clf = SGDClassifier()\n",
    "\n",
    "sgd_clf_cv = GridSearchCV(sgd_clf, param_grid, scoring='accuracy', cv=3)\n",
    "sgd_clf_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = sgd_clf_cv.best_params_\n",
    "\n",
    "print(\"Best Score (accuracy): %f\" % sgd_clf_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Optimal SGDClassifier without Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=3000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=1e-08,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDClassifier(**params_optimal, loss='log', penalty='l2')\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Optimal SGDClassifier without Early Stopping on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of Iterations: 59\n",
      "\n",
      "Weight Coefficients:\n",
      " [[-3.82347862 -1.89190411]\n",
      " [ 2.28160504 -4.97447388]\n",
      " [ 9.00509176 11.00458079]]\n",
      "\n",
      "Weight Intercept:\n",
      " [ 11.68539626  -3.64540105 -65.62785934]\n",
      "\n",
      "Accuracy:  0.9666666666666667\n",
      "\n",
      "Confusion Matrix (Test Data):\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  1 10]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNo. of Iterations:\", sgd.n_iter_ )\n",
    "\n",
    "print(\"\\nWeight Coefficients:\\n\", sgd.coef_ )\n",
    "\n",
    "print(\"\\nWeight Intercept:\\n\", sgd.intercept_ )\n",
    "\n",
    "\n",
    "y_test_predict = sgd.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predict == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (Test Data):\\n\", confusion_matrix(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent With Early Stopping\n",
    "\n",
    "\n",
    "We will implement the Logistic Regression SGDClassifier with early stopping.\n",
    "\n",
    "First, we need to find the optimal hyperparameters via Gridsearch.\n",
    "\n",
    "\n",
    "## Model Selection: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.966667\n",
      "Optimal Hyperparameter Values:  {'max_iter': 7000, 'alpha': 0.01, 'n_iter_no_change': 10, 'tol': 1e-05}\n",
      "CPU times: user 3.12 s, sys: 21.3 ms, total: 3.14 s\n",
      "Wall time: 3.18 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {'alpha': [0.1, 0.01, 0.001, 0.0005, 0.0001],'n_iter_no_change' : [10, 15, 20, 25],\n",
    "              'tol': [1e-3, 1e-5, 1e-8], 'max_iter':[500, 1000, 3000, 7000]}\n",
    "              \n",
    "\n",
    "\n",
    "\n",
    "sgd_clf = SGDClassifier()\n",
    "\n",
    "sgd_clf_cv = GridSearchCV(sgd_clf, param_grid, scoring='accuracy', cv=3)\n",
    "sgd_clf_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = sgd_clf_cv.best_params_\n",
    "\n",
    "print(\"Best Score (accuracy): %f\" % sgd_clf_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Optimal SGDClassifier with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.01, average=False, class_weight=None,\n",
       "       early_stopping=True, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=7000,\n",
       "       n_iter=None, n_iter_no_change=10, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=1e-05,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDClassifier(**params_optimal, loss='log', penalty='l2', early_stopping=True, learning_rate='optimal')\n",
    "\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Optimal SGDClassifier with Early Stopping on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of Iterations: 12\n",
      "\n",
      "Weight Coefficients:\n",
      " [[-2.34153663 -1.03423035]\n",
      " [ 1.00306339 -1.42409374]\n",
      " [ 2.398042    2.47542129]]\n",
      "\n",
      "Weight Intercept:\n",
      " [  7.09114137  -2.45108669 -16.04311892]\n",
      "\n",
      "Accuracy:  1.0\n",
      "\n",
      "Confusion Matrix (Test Data):\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNo. of Iterations:\", sgd.n_iter_ )\n",
    "\n",
    "\n",
    "print(\"\\nWeight Coefficients:\\n\", sgd.coef_ )\n",
    "\n",
    "print(\"\\nWeight Intercept:\\n\", sgd.intercept_ )\n",
    "\n",
    "\n",
    "y_test_predict = sgd.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predict == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (Test Data):\\n\", confusion_matrix(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation: Early Stopping Regularization\n",
    "\n",
    "The SGDClassifier (or any gradient descent algorithm) starts with weights almost zero. Then, it moves away as training continues. If we stop the training early, as soon as the validation error reaches minimum, we get a model with more weights close to zero and effectively fewer parameters.\n",
    "\n",
    "If we compare the weight coefficients and the number iterations between the two implementations (without and with early stopping), we will see the difference. \n",
    "\n",
    "Early stopping makes the weights smaller (enables feature selection) and requires less iterations (faster training time). \n",
    "\n",
    "However, due to its stochastic nature we will not always get the optimal theta. Hence the model performance will not be as good as the gradient descent based approach.\n",
    "\n",
    "With smaller dataset (e.g., Iris), we will not see the benefit of SGDclassifier with early stopping. However, for larger datasets (e.g., MNIST handwritten digit recongnition), the SGDClassifier will improve the model performance in two ways:\n",
    "\n",
    "- Faster training time\n",
    "- Feature selection (due to early stopping as well as l2 regularization)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
