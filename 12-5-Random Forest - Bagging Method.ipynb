{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "\n",
    "Decision Trees are versatile (can do both regression and classificatiion) algorithms. \n",
    "\n",
    "Using **deep** Decision Trees we can fit complex datasets. However, it suffers from **high variance** (overfitting).\n",
    "\n",
    "One technique to reduce the high variance of Decision Trees is to train a **group of Decision Trees**, each on a different random subset of the training set. \n",
    "\n",
    "To make predictions, we just obtain the predictions of all individual trees. Then, predict the class that gets the most votes. \n",
    "\n",
    "Such an ensemble of Decision Trees is called a **Random Forest**.\n",
    "\n",
    "Despite its simplicity, this is one of the most powerful Machine Learning algorithms available today.\n",
    "\n",
    "\n",
    "## Random Forest vs. SVM & Boosting\n",
    "\n",
    "Support Vector Machines (SVMs) and boosting are very powerful for binary classification problems. However, these techniques do not extend naturally to **multi-class problems**.\n",
    "\n",
    "In principle, classification trees and forests work with any number of classes (~20 to ~30).\n",
    "\n",
    "In average, classification forests have shown good generalization, even in problems with **high dimensionality**. \n",
    "\n",
    "\n",
    "## Training Random Forests: Bagging\n",
    "\n",
    "A random decision forest is generally trained via the **bagging** method (or sometimes pasting).\n",
    "\n",
    "Bagging stands for **bootstrap aggregation**.\n",
    "\n",
    "In bootstrapping we use the same training algorithm for every predictor, but to train them on different random subsets of the training set. \n",
    "- When sampling is performed with replacement, this method is called bagging. \n",
    "- When sampling is performed without replacement, it is called pasting.\n",
    "\n",
    "\n",
    "\n",
    "## How Do Random Forests Reduce Variance to Improve Generalizability?\n",
    "\n",
    "A key aspect of random forests is the fact that its component trees are all **randomly different from one another**. \n",
    "\n",
    "This leads to **de-correlation between the individual tree predictions** and, in turn, to improved generalization.\n",
    "\n",
    "Forest randomness also helps achieve high robustness with respect to **noisy data**.\n",
    "\n",
    "\n",
    "## Randomization Techniques for Training Random Forests\n",
    "\n",
    "Randomness is injected into the trees during the training phase. \n",
    "\n",
    "Two of the most popular ways of doing so are:\n",
    "     - Random training data set sampling  (e.g., bagging) \n",
    "     - Randomized node optimization (RNO)\n",
    "\n",
    "\n",
    "Unlike bagging, RNO uses use all available training data and controls the randomness by varying the number of parameters for training.\n",
    "\n",
    "In this notebook, we will **explore the bagging method** for training Random Forests. We will also study a variant of Random Forests that is known as extremely randomized trees or **Extra-Trees**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: MNIST\n",
    "\n",
    "\n",
    "We will use the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.\n",
    "\n",
    "\n",
    "There are 70,000 images. Each image is 28x28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black).\n",
    "\n",
    "Thus, each image has 784 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Data Matrix (X) and the Label Vector (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "X, y = mnist[\"data\"], mnist[\"target\"] \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Sets\n",
    "\n",
    "The MNIST dataset is already split into a training set (the first 60,000 images) and a test set (the last 10,000 images).\n",
    "\n",
    "We will shuffle the training set to ensure that all cross-validation folds will be similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Discriminative Features Via Dimensionaly Reduction\n",
    "\n",
    "It is useful to **find disciminative features** for growing the tree. For this we could do feature selection or feature extraction.\n",
    "\n",
    "We have experimented with feature extraction implemented by dimensionality reduction.\n",
    "\n",
    "We applied the Principle Component Analysis (PCA) dimensionality reduction technique to project the MNIST dataset (784 features) to a lower dimensional space by retaining maximum variance. \n",
    "\n",
    "However, we observed that there was no performance improvement. Thus we removed the results that were based on PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growing (Training) a Random Forest\n",
    "\n",
    "\n",
    "To reduce the variance of a Decision Tree we can grow Random Forest.\n",
    "\n",
    "The Random Forest algorithm introduces extra randomness when growing trees.\n",
    "\n",
    "Instead of searching for the very best feature when splitting a node, it searches for the best feature among a **random subset of features**. This results in a greater tree diversity, which trades a higher bias for a lower variance, generally yielding an overall better model. \n",
    "\n",
    "\n",
    "A Random Forest is an **ensemble of Decision Trees**. It is generally trained via the bagging method (or sometimes pasting). \n",
    "\n",
    "Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, we can instead use the **RandomForestClassifier class**, which is more convenient and optimized for Decision Trees (similarly, there is a RandomForestRegressor class for regression tasks). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest: Key Hyperparameters\n",
    "\n",
    "\n",
    "We will use the following hyperparameters of the RandomForestClassifier class. \n",
    "\n",
    "For a full list of the hyperparameters visit: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "\n",
    "- n_estimators : The number of trees in the forest. Default=10\n",
    "\n",
    "\n",
    "- criterion : The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific. Default=”gini”\n",
    "\n",
    "\n",
    "- max_depth : The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Default=None\n",
    "\n",
    "\n",
    "- min_samples_split : The minimum number of samples required to split an internal node: Default=2\n",
    "\n",
    "        -- If int, then consider min_samples_split as the minimum number.\n",
    "        -- If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "        \n",
    "        \n",
    "        \n",
    "- min_samples_leaf : The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. Default=1\n",
    "\n",
    "\n",
    "         -- If int, then consider min_samples_leaf as the minimum number.\n",
    "         -- If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "\n",
    "\n",
    "- max_features : The number of features to consider when looking for the best split. Default=\"auto\".\n",
    "\n",
    "        -- If int, then consider max_features features at each split.\n",
    "        -- If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "        -- If “auto”, then max_features=sqrt(n_features).\n",
    "        -- If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "        -- If “log2”, then max_features=log2(n_features).\n",
    "        -- If None, then max_features=n_features.\n",
    "\n",
    "\n",
    "- max_leaf_nodes : Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. Default=None\n",
    "\n",
    "\n",
    "- class_weight : Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Default=None\n",
    "\n",
    "         -- The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "\n",
    "\n",
    "- oob_score : Whether to use out-of-bag samples to estimate the generalization accuracy. Default=False\n",
    "\n",
    "        -- When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting oob_score=True. Then, we can get the score of the training dataset obtained using an out-of-bag estimate (use the $oob\\_score\\_$ attribute)\n",
    "        \n",
    "        \n",
    "- bootstrap : Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. Default=True\n",
    " \n",
    "\n",
    "- verbose : Controls the verbosity when fitting and predicting. Default=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   55.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9715\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 971    0    0    0    0    2    2    1    3    1]\n",
      " [   0 1123    3    3    0    2    2    0    1    1]\n",
      " [   6    0 1001    5    3    0    3    8    6    0]\n",
      " [   0    0    9  975    0    5    0    9    8    4]\n",
      " [   1    0    3    0  958    0    4    0    2   14]\n",
      " [   3    0    1   11    3  860    5    2    4    3]\n",
      " [   6    3    0    0    2    3  939    0    5    0]\n",
      " [   1    3   17    1    1    0    0  993    1   11]\n",
      " [   2    0    6    7    3    5    3    3  935   10]\n",
      " [   5    5    2   13   12    3    1    5    3  960]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98       980\n",
      "         1.0       0.99      0.99      0.99      1135\n",
      "         2.0       0.96      0.97      0.97      1032\n",
      "         3.0       0.96      0.97      0.96      1010\n",
      "         4.0       0.98      0.98      0.98       982\n",
      "         5.0       0.98      0.96      0.97       892\n",
      "         6.0       0.98      0.98      0.98       958\n",
      "         7.0       0.97      0.97      0.97      1028\n",
      "         8.0       0.97      0.96      0.96       974\n",
      "         9.0       0.96      0.95      0.95      1009\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "\n",
      "Score of the training dataset obtained using an out-of-bag estimate:  0.9709666666666666\n",
      "CPU times: user 13min 54s, sys: 18.3 s, total: 14min 12s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", max_features=\"auto\", \n",
    "                                    verbose=1, max_depth=32, class_weight=\"balanced\", oob_score=True, n_jobs=-1)\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "y_test_predicted = forest_clf.predict(X_test)\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nScore of the training dataset obtained using an out-of-bag estimate: \", forest_clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest: Feature Selection by Feature Importance\n",
    "\n",
    "\n",
    "**Feature Selection** is a very useful Machine Learning technique that we often use. Random Forests are very handy to get a quick understanding of what features actually matter. This helps to perform feature selection.\n",
    "\n",
    "Random Forests enable us to perform feature selection by finding the **relative importance of each feature**. \n",
    "\n",
    "A Random Forest measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). \n",
    "\n",
    "More precisely, it is a **weighted average**, where each node’s weight is equal to the number of training samples that are associated with it.\n",
    "\n",
    "Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. \n",
    "\n",
    "We can access the result using the $feature\\_importances\\_$ variable. \n",
    "\n",
    "Following example shows the plot of each pixel's importance in the MNIST dataset after trained by a Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAAF1CAYAAAAqQ9nrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pixel importances on 28*28 image\n",
    "importances = forest_clf.feature_importances_\n",
    "image = importances.reshape(28, 28)\n",
    "\n",
    "# Plot pixel importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"MNIST Pixel Importance\")\n",
    "\n",
    "plt.imshow(image, cmap=mpl.cm.hot,interpolation=\"nearest\")\n",
    "cbar = plt.colorbar(ticks=[forest_clf.feature_importances_.min(), forest_clf.feature_importances_.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'])\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Trees\n",
    "\n",
    "Random Forests are used to reduce the variance of Decision Trees. \n",
    "\n",
    "Interestingly, we can **reduce the variance** of the Random Forests by growing **extremely randomized trees**. In Scikit-Learn it is implemented by the ExtraTreesClassifier and ExtraTreesRegressor classes.\n",
    "\n",
    "These Extra-Trees take randomness one step further by using **random thresholds** for each feature rather than searching for the best possible thresholds (like regular Decision Trees do).\n",
    "\n",
    "Thus Extra-Trees trade **more bias for a lower variance**.\n",
    "\n",
    "Extra-Trees are much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree.\n",
    "\n",
    "Scikit-Learn’s ExtraTreesClassifier API is identical to the RandomForestClassifier class. Similarly, the Extra TreesRegressor class has the same API as the RandomForestRegressor class.\n",
    "\n",
    "### Note:\n",
    "By deault the ExtraTreesClassifier \"bootstrap\" hyperparameter is set to False. As a consequence, the whole dataset is used to build each tree. To using the bagging method, we need to explicity set it to True.\n",
    "\n",
    "\n",
    "### RandomForestClassifier vs. ExtraTreesClassifier\n",
    "\n",
    "It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an ExtraTreesClassifier. \n",
    "\n",
    "Generally, the only way to know is to try both and compare them using cross-validation (and tuning the hyperparameters using grid search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9711\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 969    0    0    0    0    2    5    1    3    0]\n",
      " [   0 1123    3    3    0    1    3    0    1    1]\n",
      " [   7    0  995    5    4    0    4    9    8    0]\n",
      " [   1    0    8  978    0    5    0    9    6    3]\n",
      " [   1    0    2    0  953    0    6    1    2   17]\n",
      " [   3    0    0   10    1  865    6    1    4    2]\n",
      " [   7    3    0    0    3    2  942    0    1    0]\n",
      " [   1    4   19    1    0    0    0  987    1   15]\n",
      " [   5    0    3    8    4    4    2    3  938    7]\n",
      " [   6    6    2   11    9    3    1    5    5  961]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98       980\n",
      "         1.0       0.99      0.99      0.99      1135\n",
      "         2.0       0.96      0.96      0.96      1032\n",
      "         3.0       0.96      0.97      0.97      1010\n",
      "         4.0       0.98      0.97      0.97       982\n",
      "         5.0       0.98      0.97      0.98       892\n",
      "         6.0       0.97      0.98      0.98       958\n",
      "         7.0       0.97      0.96      0.97      1028\n",
      "         8.0       0.97      0.96      0.97       974\n",
      "         9.0       0.96      0.95      0.95      1009\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "\n",
      "Score of the training dataset obtained using an out-of-bag estimate:  0.97045\n",
      "CPU times: user 8min 49s, sys: 17.5 s, total: 9min 6s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=1000, criterion=\"gini\", max_features=\"auto\", \n",
    "                                    verbose=1, max_depth=32, class_weight=\"balanced\", oob_score=True, bootstrap=True, n_jobs=-1)\n",
    "extra_trees_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_test_predicted = extra_trees_clf.predict(X_test)\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nScore of the training dataset obtained using an out-of-bag estimate: \", extra_trees_clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Running-Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (1000 trees)</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>2min 48s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extra-Trees (1000 trees)</td>\n",
       "      <td>0.9711</td>\n",
       "      <td>2min</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Classifier  Accuracy Running-Time\n",
       "0  Random Forest (1000 trees)    0.9715     2min 48s\n",
       "1    Extra-Trees (1000 trees)    0.9711         2min"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[\"Random Forest (1000 trees)\", 0.9715, \"2min 48s\"], \n",
    "        [\"Extra-Trees (1000 trees)\", 0.9711, \"2min\"]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"Classifier\", \"Accuracy\", \"Running-Time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Understanding\n",
    "\n",
    "Based on the performance results we see that Extra-Trees and Random Forests are comparable. \n",
    "\n",
    "- Extra-Trees are slightly faster at the cost of slightly higher bias (i.e., slightly smaller test accuracy). \n",
    "- The performance difference is insignificant.\n",
    "\n",
    "To get a better comparative understanding we should tune the hyperparameters for these two models."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
