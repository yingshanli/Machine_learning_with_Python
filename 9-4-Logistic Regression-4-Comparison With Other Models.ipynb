{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Comparison With Other Models\n",
    "\n",
    "\n",
    "The goal of this notebook is to compare the Logistic Regression classifier model with two other classifier models that we have studied. \n",
    "\n",
    "In particular we will investigate the following three classification approaches.\n",
    "\n",
    "- Instance or Memory Based Learning (K-Nearest Neighbor)\n",
    "- Model Based Learning\n",
    "      -- Generative Approach (naive Bayes classifier)\n",
    "      -- Discrinimative Approach (Logistic Regression)\n",
    "\n",
    "\n",
    "For this study we will use Scikit-Learn's LogisticRegression object that uses the Gradient Descent algorithm. Since our dataset is small, Gradient Descent based Logistic Regression will not require longer training time. However, for larger dataset, we should use the Stochastic Gradient Descent implemetation of Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "\n",
    "We will use the iris dataset, which is a multivariate data set. \n",
    "\n",
    "This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica\n",
    "\n",
    "There are 4 features: \n",
    "- sepal length (cm)\n",
    "- sepal width (cm)\n",
    "- petal length (cm)\n",
    "- petal width (cm)\n",
    "\n",
    "Total number of samples: 150\n",
    "\n",
    "The dataset is also known as Fisher's Iris data set as it was introduced by the British statistician and biologist Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\".\n",
    "\n",
    "\n",
    "<img src=\"https://cse.unl.edu/~hasan/IrisFlowers.png\",width=800,height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['filename', 'DESCR', 'feature_names', 'target', 'data', 'target_names'])\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "print(iris.keys())\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "print(iris.data.shape)\n",
    "\n",
    "#print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Matrix (X) and the Label Vector (y)\n",
    "\n",
    "We can use all features or a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression\n",
    "\n",
    "We will use the multonimal (softmax) logistic regression for multi-class clasification.\n",
    "\n",
    "## Model Selection for Softmax Regression: Hyperparameter Tuning\n",
    "\n",
    "First, we need to find the optimal hyperparameters via Gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.966667\n",
      "Optimal Hyperparameter Values:  {'max_iter': 10000, 'solver': 'sag', 'tol': 0.001, 'C': 10, 'multi_class': 'multinomial'}\n",
      "CPU times: user 9.52 s, sys: 62.4 ms, total: 9.58 s\n",
      "Wall time: 9.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid_sm = {'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], \n",
    "              'multi_class' : ['multinomial'],\n",
    "              'tol': [1e-3, 1e-4, 1e-5], 'max_iter':[10000, 20000],'C': [10, 50]}\n",
    "\n",
    "lg_reg_sm = LogisticRegression()\n",
    "\n",
    "lg_reg_sm_cv = GridSearchCV(lg_reg_sm, param_grid_sm, scoring='accuracy', cv=3)\n",
    "lg_reg_sm_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal_sm = lg_reg_sm_cv.best_params_\n",
    "\n",
    "print(\"Best Score (accuracy): %f\" % lg_reg_sm_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train the Optimal Softmax Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg = LogisticRegression(**params_optimal_sm)\n",
    "\n",
    "softmax_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluate the Optimal Softmax Logistic Regression Classifier on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Iterations: [132]\n",
      "\n",
      "Accuracy:  1.0\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Iterations:\", softmax_reg.n_iter_ )\n",
    "\n",
    "\n",
    "y_test_predicted = softmax_reg.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. K-Nearest Neighbor\n",
    "\n",
    "## Model Selection for KNN: Hyperparameter Tuning\n",
    "\n",
    "First, we need to find the optimal hyperparameters via Gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.974941\n",
      "Optimal Hyperparameter Values:  {'weights': 'distance', 'p': 10, 'n_neighbors': 43}\n",
      "CPU times: user 12.5 s, sys: 479 ms, total: 13 s\n",
      "Wall time: 11.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The param_grid tells Scikit-Learn to first evaluate all 50 × 2 = 100 combinations of \n",
    "#   n_neighbors and p hyperparameter values specified in the dict\n",
    "param_grid_knn = {'n_neighbors': np.arange(1,50), 'p': [1, 2, 10, 50, 100, 500, 1000], \n",
    "                  'weights': [\"uniform\", \"distance\"]}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn_cv = GridSearchCV(knn, param_grid_knn, scoring='f1_macro', cv=3)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal_knn = knn_cv.best_params_\n",
    "\n",
    "print(\"Best Score: %f\" % knn_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the Optimal KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 773 µs, sys: 324 µs, total: 1.1 ms\n",
      "Wall time: 845 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "knn_clf = KNeighborsClassifier(**params_optimal_knn)\n",
    "\n",
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate the Optimal KNN Classifier on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  1.0\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = knn_clf.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gaussian Naive Bayes Classifier\n",
    "\n",
    "Since the features values are real-numbers, we will use the Gaussian Naive Bayes for classification.\n",
    "\n",
    "## Note:\n",
    "There is no model selection in NB classifiers (hyperparameter tunining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussianNB_clf = GaussianNB()\n",
    "\n",
    "gaussianNB_clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Gaussian NB Classifier on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  1.0\n",
      "\n",
      "Text Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = gaussianNB_clf.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "print(\"\\nText Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Understanding of Three Classifiers\n",
    "\n",
    "### Naive Bayes Classifier\n",
    "- No model selection (hyperparameter tuning). Thus the we can immediately start training the model.\n",
    "- Works well even if data is small.\n",
    "- Scale well with high-dimensional features, hence we don't have to worry about the curse of dimensionality.\n",
    "- No need for regularization.\n",
    "- Since it's not an optimization technique, we don't have to search appropriate learning rate, generate learning curves to detect underfitting/overfitting, regularize via early stopping, etc.\n",
    "- Life is good with the NB classifier!\n",
    "- Can directly perform multi-class classification. We don't need to train multiple binary classifiers.\n",
    "\n",
    "\n",
    "### K-Nearest Neighbor\n",
    "- No learning!\n",
    "- There is no need to compute class probabilities.\n",
    "- Suffers from the curse of dimensionality.\n",
    "- Training time is painfully longer for large dataset and high-dimensional features.\n",
    "- Can directly perform multi-class classification. We don't need to train multiple binary classifiers.\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "- It's mainly a binary classifier.\n",
    "- For non-linear dataset we need to augment the features.\n",
    "- For multi-class classification we need to train multiple binary classifiers when we use the one-versus-all (OvA) strategy. However, the Softmax regression is able to directly perform multi-class classification.\n",
    "- No closed form solution!\n",
    "- Batch Gradient Descent takes longer time on larger dataset.\n",
    "- Training gets slower if the learning rate is not chosen appropriately.\n",
    "- For faster convergence we need to use 2nd order derivative of the cost function in the gradient descent algorithm.\n",
    "- Stochastic Gradient Descent requires careful hyperparparameter tuning.\n",
    "\n",
    "\n",
    "\n",
    "## Logistic Regression vs Naive Bayes\n",
    "\n",
    "- Logistic Regression makes no assumption about P(X|Y) in learning, while the Naive Bayes does!\n",
    "- They optimize different functions and obtain different solutions.\n",
    "\n",
    "In general, NB and LR make different assumptions\n",
    "- NB: Features are independent given class: P(X|Y) \n",
    "- LR: Defines the functional form of P(Y|X), but no assumptionon about the features P(X|Y)\n",
    "\n",
    "Convergence rates\n",
    "- GNB(usually) needs less data\n",
    "- LR(usually) gets in the limit\n",
    "\n",
    "With asymptotic assumption, when model correcty\n",
    "- GNB(with class independent variances) and LR produce identical classifiers \n",
    "- LR is less biased, does not assume conditional independence, therefore LR expected to outperform GNB"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
