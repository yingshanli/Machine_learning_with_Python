{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Optimization Techniques\n",
    "\n",
    "\n",
    "The goal of this notebook is to learn how to optimize Logistic Regression based clssification. We will use a large dataset to reveal the benefit of optimization techniques.\n",
    "\n",
    "We will investigate the following two optimization techniques.\n",
    "\n",
    "- Optimization Algorithms (solvers)\n",
    "- Dimensionality Reduction\n",
    "\n",
    "The dimensionality reduction technique can be generalized to other Machine Learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We will use the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.\n",
    "\n",
    "\n",
    "There are 70,000 images. Each image is 28x28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black).\n",
    "\n",
    "Thus, each image has 784 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Data Matrix (X) and the Label Vector (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "\n",
    "X, y = mnist[\"data\"], mnist[\"target\"] \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Sets\n",
    "\n",
    "The MNIST dataset is already split into a training set (the first 60,000 images) and a test set (the last 10,000 images).\n",
    "\n",
    "We will shuffle the training set to ensure that all cross-validation folds will be similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "\n",
    "\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization using Solvers\n",
    "\n",
    "Scikit-Learn's LogisticRegression class implements regularized logistic regression using the following algorithms/solvers:\n",
    "\n",
    "- liblinear library\n",
    "- newton-cg\n",
    "- lbfgs\n",
    "- sag\n",
    "- saga\n",
    "\n",
    "These solvers are best understood based on their categories.\n",
    "\n",
    "Solvers that use **First-order Gradient Descent** Algorithm:\n",
    "- liblinear\n",
    "- sag \n",
    "- saga\n",
    "\n",
    "The \"sag\" and \"saga\" solvers implement **stochastic gradient descent**, hence they are faster than \"liblinear\".\n",
    "\n",
    "Solvers that use **Second-order Gradient Descent** Algorithm:\n",
    "- newton-cg\n",
    "- lbfgs\n",
    "\n",
    "The Second-order Gradient Descent algorithm use the information about the curvature of the space (i.e., the Hessian of the cost function) and thus converge faster. The primary example of this type of algorithm is Newton’s algorithm. Unfortunately, it may be too expensive to compute the Hessian explicitly in Newton's algorithm. \n",
    "\n",
    "To remedy this issue, the Quasi-Newton methods are created that iteratively build up an approximation to the Hessian using information gleaned from the gradient vector at each step. The most common method is called BFGS (named after its inventors, Broyden, Fletcher, Goldfarb and Shanno), which updates the approximation. There is a memory efficient version of BFGS wgich is called the limited memory BFGS, or L-BFGS.\n",
    "\n",
    "\n",
    "We will investigate the performance of these solvers on the MNIST dataset.\n",
    "\n",
    "Note that the best multi-class classification technique using Logistic Regression is the softmax regression. Only the liblinear solver doesn't support softmax regression. We have to use the one-vs-rest strategy for this solver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization using Solvers: Liblinear Library\n",
    "\n",
    "The solver “liblinear” uses a coordinate descent (CD) algorithm.\n",
    "\n",
    "The CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model. In other words, the liblinear solver does not support softmax regression for multi-class classification.\n",
    "\n",
    "It uses “one-vs-rest (ovr)” strategy to train separate binary classifiers are trained for all classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model Using the liblinear Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42min 40s, sys: 4.28 s, total: 42min 44s\n",
      "Wall time: 42min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "log_reg_ovr_liblinear = LogisticRegression(solver='liblinear')\n",
    "\n",
    "log_reg_ovr_liblinear.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the liblinear Based Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Iterations: [100]\n",
      "\n",
      "Accuracy:  0.9169\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 960    0    2    1    0    3    7    2    3    2]\n",
      " [   0 1107    4    3    0    1    5    1   13    1]\n",
      " [   9    9  903   20    7    4   13   15   49    3]\n",
      " [   4    0   18  918    2   23    5   10   23    7]\n",
      " [   1    2    4    5  909    0    9    3   10   39]\n",
      " [   9    1    0   39   10  766   16    7   34   10]\n",
      " [   8    4    8    0    5   19  907    0    7    0]\n",
      " [   3    7   24    6    8    2    1  940    5   32]\n",
      " [  12   14    6   21   12   20   10   12  857   10]\n",
      " [   8    8    2   14   28    8    0   25   14  902]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.98      0.96       980\n",
      "         1.0       0.96      0.98      0.97      1135\n",
      "         2.0       0.93      0.88      0.90      1032\n",
      "         3.0       0.89      0.91      0.90      1010\n",
      "         4.0       0.93      0.93      0.93       982\n",
      "         5.0       0.91      0.86      0.88       892\n",
      "         6.0       0.93      0.95      0.94       958\n",
      "         7.0       0.93      0.91      0.92      1028\n",
      "         8.0       0.84      0.88      0.86       974\n",
      "         9.0       0.90      0.89      0.90      1009\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Iterations:\", log_reg_ovr_liblinear.n_iter_ )\n",
    "\n",
    "\n",
    "y_test_predicted = log_reg_ovr_liblinear.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization using Solvers: SAG\n",
    "\n",
    "The “sag” solver uses a Stochastic Average Gradient descent. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model Using the sag Solver¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 15s, sys: 444 ms, total: 3min 16s\n",
      "Wall time: 3min 16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "softmax_reg_sag = LogisticRegression(solver='sag', multi_class='multinomial')\n",
    "\n",
    "softmax_reg_sag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the sag Based Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Iterations: [100]\n",
      "\n",
      "Accuracy:  0.9255\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 956    0    1    4    1    5    5    3    4    1]\n",
      " [   0 1116    6    2    0    1    3    1    6    0]\n",
      " [   5   13  926   16    9    3   11    7   39    3]\n",
      " [   4    1   17  925    1   24    3    9   20    6]\n",
      " [   1    2    5    4  917    0    9    6   10   28]\n",
      " [  11    3    3   39   10  765   15    7   34    5]\n",
      " [   9    3    6    3    5   17  913    1    1    0]\n",
      " [   1    6   23    9    5    1    0  947    3   33]\n",
      " [   8   13    5   22    6   22    8   13  866   11]\n",
      " [   8    7    2    8   23    6    0   21   10  924]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.98      0.96       980\n",
      "         1.0       0.96      0.98      0.97      1135\n",
      "         2.0       0.93      0.90      0.91      1032\n",
      "         3.0       0.90      0.92      0.91      1010\n",
      "         4.0       0.94      0.93      0.94       982\n",
      "         5.0       0.91      0.86      0.88       892\n",
      "         6.0       0.94      0.95      0.95       958\n",
      "         7.0       0.93      0.92      0.93      1028\n",
      "         8.0       0.87      0.89      0.88       974\n",
      "         9.0       0.91      0.92      0.91      1009\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Iterations:\", softmax_reg_sag.n_iter_ )\n",
    "\n",
    "\n",
    "y_test_predicted = softmax_reg_sag.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization using Solvers: SAGA\n",
    "\n",
    "The “saga” solver is a variant of “sag” that also supports the non-smooth penalty=”l1” option. This is therefore the solver of choice for sparse multinomial logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model Using the saga Solver¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 47s, sys: 447 ms, total: 4min 48s\n",
      "Wall time: 4min 48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "softmax_reg_saga = LogisticRegression(solver='saga', multi_class='multinomial')\n",
    "\n",
    "softmax_reg_saga.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the saga Based Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Iterations: [100]\n",
      "\n",
      "Accuracy:  0.9257\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 960    0    0    3    1    4    6    3    2    1]\n",
      " [   0 1117    5    2    0    1    3    1    6    0]\n",
      " [   5   10  925   17    9    3   11   11   39    2]\n",
      " [   4    0   17  926    1   22    2    9   22    7]\n",
      " [   1    2    5    3  917    0    9    6   10   29]\n",
      " [  12    3    2   37   10  767   14    7   35    5]\n",
      " [   9    3    7    3    6   17  910    2    1    0]\n",
      " [   1    6   23    7    5    1    0  947    4   34]\n",
      " [   8   13    5   23    6   23    9   11  864   12]\n",
      " [   9    7    1    8   23    7    0   21    9  924]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.98      0.97       980\n",
      "         1.0       0.96      0.98      0.97      1135\n",
      "         2.0       0.93      0.90      0.91      1032\n",
      "         3.0       0.90      0.92      0.91      1010\n",
      "         4.0       0.94      0.93      0.94       982\n",
      "         5.0       0.91      0.86      0.88       892\n",
      "         6.0       0.94      0.95      0.95       958\n",
      "         7.0       0.93      0.92      0.93      1028\n",
      "         8.0       0.87      0.89      0.88       974\n",
      "         9.0       0.91      0.92      0.91      1009\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Iterations:\", softmax_reg_saga.n_iter_ )\n",
    "\n",
    "\n",
    "y_test_predicted = softmax_reg_saga.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization using Solvers: Newton-CG\n",
    "\n",
    "The solver “newton-cg” uses implements second order optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model Using the newton-cg Solver¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 3min 30s, sys: 2min 32s, total: 2h 6min 2s\n",
      "Wall time: 31min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "softmax_reg_newtoncg = LogisticRegression(solver='newton-cg', multi_class='multinomial')\n",
    "\n",
    "softmax_reg_newtoncg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the newton-cg Based Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Iterations: [100]\n",
      "\n",
      "Accuracy:  0.9206\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 953    0    6    2    1    6    5    4    3    0]\n",
      " [   0 1110    8    3    0    1    3    2    8    0]\n",
      " [  13   16  908   20   10    5   11    7   37    5]\n",
      " [   4    1   22  920    2   21    2   11   21    6]\n",
      " [   3    4    9    5  908    0    8    6    8   31]\n",
      " [  10    3    2   35   10  770   15    8   34    5]\n",
      " [  10    4   11    2    7   17  906    0    1    0]\n",
      " [   1   10   24    7    7    2    0  943    5   29]\n",
      " [   8   16    4   22    6   24    9   10  863   12]\n",
      " [   6    7    2    9   20    8    1   21   10  925]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.97      0.96       980\n",
      "         1.0       0.95      0.98      0.96      1135\n",
      "         2.0       0.91      0.88      0.90      1032\n",
      "         3.0       0.90      0.91      0.90      1010\n",
      "         4.0       0.94      0.92      0.93       982\n",
      "         5.0       0.90      0.86      0.88       892\n",
      "         6.0       0.94      0.95      0.94       958\n",
      "         7.0       0.93      0.92      0.92      1028\n",
      "         8.0       0.87      0.89      0.88       974\n",
      "         9.0       0.91      0.92      0.91      1009\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Iterations:\", softmax_reg_newtoncg.n_iter_ )\n",
    "\n",
    "\n",
    "y_test_predicted = softmax_reg_newtoncg.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization using Solvers: L-BFGS\n",
    "\n",
    "The “lbfgs” is an optimization algorithm that approximates the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm, which belongs to quasi-Newton methods. \n",
    "\n",
    "The “lbfgs” solver is recommended for use for small data-sets but for larger datasets its performance might suffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model Using the lbfgs Solver¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 3.15 s, total: 1min 12s\n",
      "Wall time: 18.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "softmax_reg_lbfgs = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "softmax_reg_lbfgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the lbfgs Based Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Iterations: [100]\n",
      "\n",
      "Accuracy:  0.9255\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 963    0    0    3    1    3    4    4    2    0]\n",
      " [   0 1112    4    2    0    1    3    2   11    0]\n",
      " [   3   10  926   15    6    4   15    8   42    3]\n",
      " [   4    1   21  916    1   26    3    9   22    7]\n",
      " [   1    1    7    3  910    0    9    7   10   34]\n",
      " [  11    2    1   33   11  776   11    6   35    6]\n",
      " [   9    3    7    3    7   16  910    2    1    0]\n",
      " [   1    6   24    5    7    1    0  951    3   30]\n",
      " [   8    7    6   23    6   26   10   10  869    9]\n",
      " [   9    7    0   11   25    6    0   22    7  922]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.98      0.97       980\n",
      "         1.0       0.97      0.98      0.97      1135\n",
      "         2.0       0.93      0.90      0.91      1032\n",
      "         3.0       0.90      0.91      0.91      1010\n",
      "         4.0       0.93      0.93      0.93       982\n",
      "         5.0       0.90      0.87      0.89       892\n",
      "         6.0       0.94      0.95      0.95       958\n",
      "         7.0       0.93      0.93      0.93      1028\n",
      "         8.0       0.87      0.89      0.88       974\n",
      "         9.0       0.91      0.91      0.91      1009\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Iterations:\", softmax_reg_lbfgs.n_iter_ )\n",
    "\n",
    "\n",
    "y_test_predicted = softmax_reg_lbfgs.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Comparison of the Solvers\n",
    "\n",
    "We observe that the saga solver has the best accuracy.\n",
    "\n",
    "However, lbfgs is much faster than other solvers and its accuracy is comparable to saga.\n",
    "\n",
    "Thus, **lbfgs is the best performing solver**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Solver</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Running-Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.9169</td>\n",
       "      <td>42min 47s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sag</td>\n",
       "      <td>0.9255</td>\n",
       "      <td>3min 16s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saga</td>\n",
       "      <td>0.9257</td>\n",
       "      <td>4min 48s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.9206</td>\n",
       "      <td>31min 40s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.9255</td>\n",
       "      <td>18.4 s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Solver  Accuracy Running-Time\n",
       "0  liblinear    0.9169    42min 47s\n",
       "1        sag    0.9255     3min 16s\n",
       "2       saga    0.9257     4min 48s\n",
       "3  newton-cg    0.9206    31min 40s\n",
       "4      lbfgs    0.9255       18.4 s"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = [[\"liblinear\", 0.9169, \"42min 47s\"], \n",
    "        [\"sag\", 0.9255, \"3min 16s\"],\n",
    "        [\"saga\", 0.9257, \"4min 48s\"],\n",
    "        [\"newton-cg\", 0.9206, \"31min 40s\"],\n",
    "        [\"lbfgs\", 0.9255, \"18.4 s\"]]\n",
    "pd.DataFrame(data, columns=[\"Solver\", \"Accuracy\", \"Running-Time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Using Dimensionaly Reduction\n",
    "\n",
    "We can optimize the running-time of the Logistic Regression algorithm by reducing the number of features. Our assumption is that the essence or core content of the data does not span along all dimensions. The technique for reducing the dimension of data is known as dimensionality reduction.\n",
    "\n",
    "For a gentle introduction to various dimensionality reduction technique, see the notebook \"Dimensionality Reduction\" in the Github repository.\n",
    "\n",
    "We will use the Principle Component Analysis (PCA) dimensionality reduction technique to project the MNIST dataset (784 features) to a lower dimensional space by retaining maximum variance. \n",
    "\n",
    "The goal is to see the improvement in training time due to this dimensionality reduction.\n",
    "\n",
    "Before we apply the PCA, we need to standardize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Data\n",
    "\n",
    "PCA is influenced by scale of the data. Thus we need to scale the features of the data before applying PCA. \n",
    "\n",
    "For understanding the negative effect of not scaling the data, see the following post:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py\n",
    "\n",
    "Note that we fit the scaler on the training set and transform on the training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA\n",
    "\n",
    "While applying PCA we can set the number of principle components by the \"n_components\" attribute. But more importantly, we can use this attribute to determine the % of variance we want to retain in the extracted features.\n",
    "\n",
    "For example, if we set it to 0.95, sklearn will choose the **minimum number of principal components** such that 95% of the variance is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Principle Components\n",
    "\n",
    "We can find how many components PCA chose after fitting the model by using the following attribute: n_components_\n",
    "\n",
    "We will see that 95% of the variance amounts to **315 principal components**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numberof Principle Components:  315\n"
     ]
    }
   ],
   "source": [
    "print(\"Numberof Principle Components: \", pca.n_components_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Mapping (Transform) to both the Training Set and the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Logistic Regression Model\n",
    "\n",
    "We use the best performing solver (i.e., lbfgs) to train the model on the PCA transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.1 s, sys: 1.86 s, total: 32 s\n",
      "Wall time: 8.04 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "softmax_reg_pca = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "softmax_reg_pca.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model on the Test Data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Iterations: [100]\n",
      "\n",
      "Accuracy:  0.9265\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 957    0    1    2    1    6    8    3    2    0]\n",
      " [   0 1114    3    2    0    1    3    2   10    0]\n",
      " [   7    5  931   17   12    3    9   11   34    3]\n",
      " [   3    3   18  919    1   22    3   11   23    7]\n",
      " [   1    2    8    2  917    0   10    4    9   29]\n",
      " [   7    5    3   33    8  778   13    6   35    4]\n",
      " [  12    3    8    2    6   12  912    1    2    0]\n",
      " [   0    9   29    5    6    1    0  948    0   30]\n",
      " [   6    6    6   22    9   24    7   11  874    9]\n",
      " [   9    7    2   10   25    7    0   26    8  915]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97       980\n",
      "         1.0       0.97      0.98      0.97      1135\n",
      "         2.0       0.92      0.90      0.91      1032\n",
      "         3.0       0.91      0.91      0.91      1010\n",
      "         4.0       0.93      0.93      0.93       982\n",
      "         5.0       0.91      0.87      0.89       892\n",
      "         6.0       0.95      0.95      0.95       958\n",
      "         7.0       0.93      0.92      0.92      1028\n",
      "         8.0       0.88      0.90      0.89       974\n",
      "         9.0       0.92      0.91      0.91      1009\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Iterations:\", softmax_reg_pca.n_iter_ )\n",
    "\n",
    "\n",
    "y_test_predicted = softmax_reg_pca.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Comparison of All Optimization Techniques\n",
    "\n",
    "We observe that after performing PCA (retaining 95% variance), we achieve\n",
    "- Best accuracy\n",
    "- Smallest running-time\n",
    "\n",
    "Thus, the best way to optimize the Logistic Regression model is to use the best performing solver (it would vary depending on the dataset) and by applying dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Solver</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Running-Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.9169</td>\n",
       "      <td>42min 47s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sag</td>\n",
       "      <td>0.9255</td>\n",
       "      <td>3min 16s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saga</td>\n",
       "      <td>0.9257</td>\n",
       "      <td>4min 48s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.9206</td>\n",
       "      <td>31min 40s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.9255</td>\n",
       "      <td>18.4 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PCA+lbfgs</td>\n",
       "      <td>0.9265</td>\n",
       "      <td>8.04 s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Solver  Accuracy Running-Time\n",
       "0  liblinear    0.9169    42min 47s\n",
       "1        sag    0.9255     3min 16s\n",
       "2       saga    0.9257     4min 48s\n",
       "3  newton-cg    0.9206    31min 40s\n",
       "4      lbfgs    0.9255       18.4 s\n",
       "5  PCA+lbfgs    0.9265       8.04 s"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = [[\"liblinear\", 0.9169, \"42min 47s\"], \n",
    "        [\"sag\", 0.9255, \"3min 16s\"],\n",
    "        [\"saga\", 0.9257, \"4min 48s\"],\n",
    "        [\"newton-cg\", 0.9206, \"31min 40s\"],\n",
    "        [\"lbfgs\", 0.9255, \"18.4 s\"],\n",
    "        [\"PCA+lbfgs\", 0.9265, \"8.04 s\"]]\n",
    "pd.DataFrame(data, columns=[\"Solver\", \"Accuracy\", \"Running-Time\"])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
